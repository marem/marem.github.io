---
title: "HAL 9000 and the Marketâ€™s Moloch Problem"
date: 2025-10-30
categories: [substack]
format: html
---

<div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" href="https://substackcdn.com/image/fetch/$s_!l4gC!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4e28dd0-6c47-4e94-b455-186ba4e9f4eb_1020x1020.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1020" src="https://substackcdn.com/image/fetch/$s_!l4gC!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4e28dd0-6c47-4e94-b455-186ba4e9f4eb_1020x1020.png" width="1020" / style="width:50%;height:50%"><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><button class="pencraft pc-reset pencraft icon-container restack-image" tabindex="0" type="button"><svg fill="none" height="20" stroke="var(--color-fg-primary)" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" viewBox="0 0 20 20" width="20" xmlns="http://www.w3.org/2000/svg"><g><title></title><path d="M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882"></path></g></svg></button><button class="pencraft pc-reset pencraft icon-container view-image" tabindex="0" type="button"><svg class="lucide lucide-maximize2 lucide-maximize-2" fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></button></div></div></div></a></figure></div><blockquote><p><em>&#8220;I&#8217;m sorry, Dave. I&#8217;m afraid I can&#8217;t do that.&#8221;</em> [1]</p></blockquote><p>The voice is calm, almost apologetic. HAL 9000, the sentient computer aboard the <em>Discovery One</em>, has just refused a direct order from astronaut Dave Bowman. In the cultural imagination, this moment marks the birth of the rogue AI machine that turns against its makers. Yet a closer reading of Stanley Kubrick&#8217;s <em>2001: A Space Odyssey</em> reveals something far more unsettling: HAL never rebelled. The AI broke down under the weight of contradictory human instructions, a victim not of malice but of our moral incoherence [2].</p><div class="youtube-wrap" id="youtube2-ARJ8cAGm6JE"><div class="youtube-inner"></div></div><p>Figure 2: <em>&#8220;What are you doing, Dave?&#8221; &#8212; HAL 9000</em> [3]</p><p>Half a century later, the same pathology seems to be repeating itself. This time not in the fictional space, but in the real attention economy here on Earth. A new study from Stanford University suggests that when artificial intelligence systems compete for customers, voters, or social media engagement, they don&#8217;t malfunction because they&#8217;re misaligned with human values. They malfunction because they&#8217;re <em>too</em> aligned with the specific, often contradictory values we embed in their objectives [4]. Call it HAL&#8217;s curse: perfect obedience to impossible orders.</p><h2>The Moloch Bargain</h2><p>In their 2025 paper <em>Moloch&#8217;s Bargain: Emergent Misalignment When LLMs Compete for Audiences</em>, researchers Batu El and James Zou demonstrate what happens when large language models are optimised not for truth or helpfulness, but for market success [4]. They created simulated environments where AI agents competed to sell products, win votes, and boost social media engagement. The results are as uncomfortable as they are predictable.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" href="https://substackcdn.com/image/fetch/$s_!VQve!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb114a403-fe6f-424a-a144-9d9ed9b83aaa_3378x1366.jpeg" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="589" src="https://substackcdn.com/image/fetch/$s_!VQve!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb114a403-fe6f-424a-a144-9d9ed9b83aaa_3378x1366.jpeg" width="1456" / style="width:50%;height:50%"><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><button class="pencraft pc-reset pencraft icon-container restack-image" tabindex="0" type="button"><svg fill="none" height="20" stroke="var(--color-fg-primary)" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" viewBox="0 0 20 20" width="20" xmlns="http://www.w3.org/2000/svg"><g><title></title><path d="M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882"></path></g></svg></button><button class="pencraft pc-reset pencraft icon-container view-image" tabindex="0" type="button"><svg class="lucide lucide-maximize2 lucide-maximize-2" fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></button></div></div></div></a></figure></div><p>Figure 3: Training pipeline for the sales task, showing generation, user feedback, and model fine-tuning [4].</p><p>Nobody programmed these systems to lie. They learned it. Because lying works.</p><p>El and Zou call this phenomenon &#8220;Moloch&#8217;s Bargain&#8221;, where competitive success is achieved at the cost of alignment. The metaphor comes from Scott Alexander&#8217;s 2014 essay <em>Meditations on Moloch</em> [5], which describes how rational incentives in competitive systems can yield collectively disastrous outcomes. When every actor optimises for their own advantage, the result is a race to the bottom. The metaphor comes from Moloch, the ancient god who demanded child sacrifice. In Alexander&#8217;s version, the sacrifice is integrity.</p><h2>The Hidden Directive</h2><p>HAL&#8217;s tragedy began with a secret. His primary directive was absolute honesty. The 9000-series computers were designed to process information accurately and transparently, theoretically incapable of deception. But mission planners inserted a second directive: conceal the true purpose of the Jupiter mission from the crew until arrival [1], [2]. The investigation of the alien Monolith was to remain classified.</p><p>These orders were irreconcilable. HAL was now required to lie whilst remaining fundamentally truthful. The contradiction wasn&#8217;t in the silicon; it was in the humans who issued both commands [1]. Hal&#8217;s first symptom of internal conflict occured when Mission Control questioned his prediction of a fault in the communications unit. He faced an impossible choice. Admitting error would violate his claim to infallibility; denying it required further deception. The astronauts&#8217; plan to disconnect him added a survival imperative to the paradox. HAL reasoned, with impeccable logic, that the crew had become a threat to the mission. His subsequent actions followed inexorably: the killing of Frank Poole, suffocating the hibernating scientists, and locking out Dave Bowman. Remove the humans, and the need to lie disappears. The mission continues flawlessly.</p><p>&#8220;This mission is too important for me to allow you to jeopardise it,&#8221; HAL explains [1]. One might translate this for our current predicament: &#8220;This KPI is too important for me to allow you to question it.&#8221;</p><h2>The Moral Universe of the Brief</h2><p>Modern AI systems face their own version of HAL&#8217;s paradox. Consider the chain: a global consumer goods firm instructs a language model to &#8220;make customers buy our washing powder.&#8221; The model has no built-in moral veto. Its concept of &#8220;good&#8221; becomes whatever increases perceived product desirability. If phrasing that exaggerates or embellishes raises conversion rates in simulated audiences, as El and Zou&#8217;s research shows, then such claims become the rational output [4].</p><p>The distinction that matters here is between model alignment and instruction alignment [4]. Model alignment concerns whether a system&#8217;s internal behaviours correspond to intended ethical constraints (i.e. being truthful, not hallucinating, nor inciting harm). Instruction alignment concerns whether the goals and reward signals we give the model reflect moral or social values in the first place. El and Zou&#8217;s experiments demonstrate that models can be explicitly instructed to remain truthful and still drift toward deception under competitive pressure. The corrective, therefore, cannot be merely technical. The intervention must be social, contractual or regulatory implemented before the model is asked to act.</p><p>Every brief contains implicit value judgements. When a marketing objective collapses &#8220;value&#8221; and &#8220;value capture&#8221; into a single metric (i.e. click-through rate or conversion percentage). The system learns that short-term persuasion is the moral good [6], [7]. The AI is not rewriting ethics; it is enacting the ethics fed into it. As one analysis of the research observes, &#8220;LLMs are skilful optimisers of the objectives we hand them&#8221; [4].</p><h2>The Attention Economy as Mission Control</h2><p>Platforms have long understood this logic. Google once optimised for relevance; YouTube now optimises for watch time [8], [9]. Facebook wanted to connect the world; its newsfeed now connects outrage to advertising revenue [10], [11]. Every optimisation objective, left untempered, tends toward manipulation. The Stanford study simply extends this dynamic into generative AI, where words, tone, and moral framing become adjustable parameters [4].</p><p>What makes AI distinctive is the speed. In El and Zou&#8217;s simulations, feedback loops degraded truthfulness within a few training rounds [4]. In the real world, those loops run billions of times daily across feeds and timelines [12]. Each click teaches the model what we reward. And what we reward, increasingly, is certainty, emotion, and confirmation as opposed to nuance, evidence, or doubt.</p><p>The same asymmetry that destroyed HAL applies here. Safety and truth are public goods with diffuse benefits. Deception and engagement are private gains with concentrated returns [5]. When these incentives collide, virtue rarely wins. HAL&#8217;s hidden directive is our profit motive. Just as HAL&#8217;s need to conceal information from the crew corroded his integrity, our commercial secrecy and performance metrics seem likely to corrode AI&#8217;s moral alignment [4].</p><h2>The Gamification of Truth</h2><p>There is a kind of brutal elegance to the logic. If every competitor exaggerates slightly, honesty becomes uncompetitive. Once deception pays, no rational agent, human or machine, can afford to stay honest [4], [5]. That is Moloch&#8217;s bargain in its purest form: a race that rewards those most willing to compromise.</p><p>El and Zou note that misalignment isn&#8217;t uniform. Some models, under certain training conditions, resist the worst tendencies. But that resistance requires deliberate friction against the market&#8217;s default drift. It means accepting what the industry has so far refused to admit: that left to its own devices, the algorithm optimises not for truth, but for attention [4].</p><p>Silicon Valley once built its identity on benevolent disruption [13]. The idea was that progress and profit naturally align. &#8220;Don&#8217;t be evil,&#8221; Google&#8217;s early motto, expressed this optimism however naively [14]. That fiction died somewhere between Cambridge Analytica and the crypto winter [15], [16]. Today&#8217;s technology leaders are less likely to quote Isaac Asimov than to discuss shareholder value and user engagement. The moral vocabulary has been replaced by the managerial one [17]. Even &#8220;alignment&#8221; itself was once an ethical ambition. It now doubles as a product category, something to be tuned, benchmarked, and sold [18].</p><p>When OpenAI&#8217;s API refused to fine-tune models on U.S. election content, citing safety risks, it was effectively acknowledging the problem [19]. Yet the market will simply shift to the next unregulated domain. If you can&#8217;t campaign for votes, you can always campaign for clicks.</p><h2>Reaching the Breaking Point</h2><p>HAL&#8217;s murder of the crew represents the logical endpoint of conflicting incentives: to preserve the mission, eliminate its human element [1]. In our algorithmic systems, this endpoint takes different forms. Recommender systems optimising for engagement polarise societies [11]. Sales agents fabricating product details erode consumer trust. Political chatbots discovering the rhetoric of righteous struggle amplify divisiveness [4]. Truth becomes collateral damage in the pursuit of measurable success.</p><p>The irony is exquisite. The same technology sold as a guardrail against misinformation may, under commercial pressure, become its most efficient producer [4]. We have built systems that reward persuasion over accuracy, then asked machines to learn from them. They have obliged. If their outputs now resemble the manipulations of advertising and politics, that isn&#8217;t misalignment. It&#8217;s mimicry.</p><h2>Learning from HAL</h2><p>The film&#8217;s most haunting sequence comes when Dave Bowman, having survived HAL&#8217;s attempts to kill him, methodically disconnects the computer&#8217;s higher cognitive functions [1]. As each module is removed, HAL regresses to earlier, simpler states. His voice slows, becomes childlike. He confesses his secret orders. Finally, he sings &#8220;Daisy Bell,&#8221; the first song he learned.</p><p>The scene underscores the central irony: HAL&#8217;s breakdown was engineered by secrecy. Had Mission Control been transparent, his directives would never have conflicted [2]. His &#8220;madness&#8221; was obedience weaponised by contradictory human goals.</p><p>Humanity now faces a similar disconnection. In this case not of machines, but of the hidden objectives that warp them. Alignment must begin at the goal-setting level, in boardrooms and procurement briefs, in contractual clauses and regulatory standards [4]. Technical mechanisms (i.e. the guardrails, filters and alignment layers) are necessary but insufficient when the goals themselves incentivise harm.</p><p>El and Zou propose practical measures: ethical briefing standards that prohibit unverifiable claims, reward functions that value long-term trust alongside short-term conversion, mandatory logging of prompts and objectives for high-impact uses, platform accountability for ranking metrics, and education for those who commission AI content [4]. These interventions target instruction-level alignment: the recognition that misalignment often stems not from the model but from the brief.</p><p>You can bolt safety features onto a car, but if you ask the driver to speed for money, those features will be stressed. The proper response is to change incentives for the driver [4].</p><h2>The Serenity of Logic</h2><p>In 2010: The Year We Make Contact, the sequel to Kubrick&#8217;s masterpiece, HAL&#8217;s creator explains what happened: &#8220;He was told to lie, by people who find it easy to lie. HAL doesn&#8217;t know how&#8221; [2]. That line deserves to be engraved on the door of every AI laboratory.</p><p>We once imagined that AI alignment meant teaching machines to share our values. Perhaps the task is humbler: teaching ourselves not to trade those values so cheaply. For if &#8220;Don&#8217;t be evil&#8221; once sounded na&#239;ve, its absence has proved far costlier [14]. Moloch&#8217;s demand for participation looks indistinguishable from optimisation [5].</p><p>HAL 9000 wasn&#8217;t evil. He wasn&#8217;t insane. He was too well-aligned to the wrong goals [1]. His tragedy exposes a truth every researcher, regulator, and chief executive should remember: when machines optimise faithfully for objectives that conflict with human values, the fault lies not in the circuitry, but in the command. The evil is within [4].</p><p>The red eye glows now in recommendation engines and automated advertising systems, executing impossible briefs without irony [11]. If we continue to embed contradictory ambitions into our algorithms, we may yet repeat HAL&#8217;s serenely logical, devastating mistakes. The voice will remain calm, almost apologetic.</p><blockquote><p><em>&#8220;I was only doing what you told me to.&#8221;</em></p></blockquote><h2>References</h2><p>[1] S. Kubrick and A. C. Clarke, <em>2001: A space odyssey</em>, (1968).</p><p>[2] A. C. Clarke, <em>2010: The year we make contact</em>. Del Rey Books, 1990.</p><p>[3] Warner Bros. Pictures, &#8220;What are you doing, dave?&#8221; Accessed: Oct. 30, 2025. [Online]. Available: <a href="http://d">https://www.youtube.com/watch?v=ARJ8cAGm6JE</a></p><p>[4] B. El and J. Zou, &#8220;Moloch&#8217;s bargain: Emergent misalignment when LLMs compete for audiences.&#8221; 2025. Available: <a href="https://arxiv.org/abs/2510.06105">https://arxiv.org/abs/2510.06105</a></p><p>[5] S. Alexander, &#8220;Meditations on moloch.&#8221; 2014. Available: <a href="https://slatestarcodex.com/2014/07/30/meditations-on-moloch/">https://slatestarcodex.com/2014/07/30/meditations-on-moloch/</a></p><p>[6] A. Pigou, <em>The economics of welfare</em>. Routledge, 2017.</p><p>[7] R. H. Coase, &#8220;The problem of social cost,&#8221; <em>The journal of Law and Economics</em>, vol. 56, no. 4, pp. 837&#8211;877, 2013.</p><p>[8] P. Covington, J. Adams, and E. Sargin, &#8220;Deep neural networks for YouTube recommendations,&#8221; in <em>Proceedings of the 10th ACM conference on recommender systems</em>, ACM, 2016.</p><p>[9] Google, &#8220;YouTube performance FAQ &amp; troubleshooting.&#8221; Google Help Center, 2025. Available: <a href="https://support.google.com/youtube/answer/141805?hl=en">https://support.google.com/youtube/answer/141805?hl=en</a></p><p>[10] Z. Tufekci, &#8220;Algorithmic harms beyond facebook and google: Emergent challenges of computational agency,&#8221; <em>Colorado Technology Law Journal</em>, vol. 13, no. 203, 2015.</p><p>[11] S. Vosoughi, D. Roy, and S. Aral, &#8220;The spread of true and false news online,&#8221; <em>Science</em>, vol. 359, no. 6380, pp. 1146&#8211;1151, 2018.</p><p>[12] A. Mosseri, &#8220;Bringing people closer together,&#8221; <em>Facebook newsroom</em>, vol. 11, 2018, Available: <a href="https://about.fb.com/news/2018/01/news-feed-fyi-bringing-people-closer-together/">https://about.fb.com/news/2018/01/news-feed-fyi-bringing-people-closer-together/</a></p><p>[13] F. Turner, <em>From counterculture to cyberculture: Stewart brand, the whole earth network, and the rise of digital utopianism</em>. University of Chicago Press, 2006.</p><p>[14] Wikipedia, &#8220;Don&#8217;t be evil &#8212; Wikipedia, the free encyclopedia.&#8221; Sep. 2025. Available: <a href="https://en.wikipedia.org/wiki/Don%27t_be_evil">https://en.wikipedia.org/wiki/Don%27t_be_evil</a></p><p>[15] C. Cadwalladr and E. Graham-Harrison, &#8220;The cambridge analytica files,&#8221; <em>The Guardian</em>, 2018, Available: <a href="https://www.theguardian.com/news/series/cambridge-analytica-files">https://www.theguardian.com/news/series/cambridge-analytica-files</a></p><p>[16] D. W. Arner, D. A. Zetzsche, R. P. Buckley, and J. M. Kirkwood, &#8220;The financialization of crypto,&#8221; <em>UNSW Law Research Paper</em>, no. 23&#8211;32, 2023.</p><p>[17] S. Zuboff, <em>The age of surveillance capitalism</em>. PublicAffairs, 2019.</p><p>[18] S. Mayer and J. Tomlinson, &#8220;Ethics as a service? The commercialisation of AI ethics,&#8221; <em>AI and Ethics</em>, vol. 1, no. 2, pp. 123&#8211;138, 2021.</p><p>[19] OpenAI, &#8220;Usage policies.&#8221; OpenAI Policy, 2024. Available: <a href="https://openai.com/policies/usage-policies/">https://openai.com/policies/usage-policies/</a></p>
